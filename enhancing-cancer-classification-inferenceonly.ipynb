{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (4.33.1)\n",
      "Requirement already satisfied: filelock in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pandas in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: gradio in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (4.1.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (5.1.2)\n",
      "Requirement already satisfied: fastapi in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.103.1)\n",
      "Requirement already satisfied: ffmpy in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.7.0)\n",
      "Requirement already satisfied: httpx in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.17.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (2.1.2)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (1.24.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (3.9.10)\n",
      "Requirement already satisfied: packaging in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (23.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (10.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (2.3.0)\n",
      "Requirement already satisfied: pydub in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: requests~=2.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (2.28.2)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (4.7.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (0.24.0.post1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio) (11.0.3)\n",
      "Requirement already satisfied: fsspec in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from gradio-client==0.7.0->gradio) (2023.9.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from huggingface-hub>=0.14.0->gradio) (3.12.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2022.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (2.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests~=2.0->gradio) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests~=2.0->gradio) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests~=2.0->gradio) (2022.12.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.6.0)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from fastapi->gradio) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: httpcore in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from httpx->gradio) (1.0.1)\n",
      "Requirement already satisfied: sniffio in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install pandas\n",
    "%pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines whether CUDA GPU or CPU is used\n",
    "device = ('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "# max length for input data\n",
    "MAX_LEN = 256\n",
    "\n",
    "# model name\n",
    "model_name = 'roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe['text']\n",
    "        self.targets = dataframe['label']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # split and rejoin sentence to standardize whitespace\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text, #text data in dataframe\n",
    "            None, #no second input\n",
    "            add_special_tokens=True, #increases accuracy\n",
    "            max_length=self.max_len, #max input\n",
    "            # truncation=True,\n",
    "            pad_to_max_length=True, #standardizes input length\n",
    "            return_token_type_ids=True #returns token ids\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        inputs returns\n",
    "        input_ids: list of token ids that represent input text\n",
    "        attention_mask: determine which tokens are input and which are padding\n",
    "        token_type_ids: token type IDs that differentiate between different segments of text\n",
    "        '''\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        # return dictionary with tensor arrays containing ids, mask, token types, and targets (values)\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RoBERTaClass(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaModel\n",
    "import torch\n",
    "\n",
    "class RoBERTaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RoBERTaClass, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\") #layer 1 has roberta model\n",
    "        self.drop = torch.nn.Dropout(0.3) #layer 2 deactivates (drops out) .30 of neurons while training\n",
    "        self.linear = torch.nn.Linear(768, 11) #weight matrix and bias vector\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids): #run model\n",
    "        _, output_1 = self.roberta(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=False\n",
    "        )\n",
    "\n",
    "        output_2 = self.drop(output_1)\n",
    "        output = self.linear(output_2)\n",
    "        return output\n",
    "\n",
    "model = RoBERTaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and run inference\n",
    "model = RoBERTaClass().to(device)\n",
    "model.load_state_dict(torch.load('enhance_cancer.pth'))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "keys = {\n",
    "    '1': 'Sustaining proliferative signaling (PS)',\n",
    "    '2': 'Evading growth suppressors (GS)',\n",
    "    '3': 'Resisting cell death (CD)',\n",
    "    '4': 'Enabling replicative immortality (RI)',\n",
    "    '5': 'Inducing angiogenesis (A)',\n",
    "    '6': ' Activating invasion & metastasis (IM)',\n",
    "    '7': 'Genome instability & mutation (GI)',\n",
    "    '8': 'Tumor-promoting inflammation (TPI)',\n",
    "    '9': 'Deregulating cellular energetics (CE)',\n",
    "    '10': 'Avoiding immune destruction (ID)'\n",
    "}\n",
    "\n",
    "def inference(symptom):\n",
    "    inference_dataset = pd.DataFrame({'text': [symptom], 'label': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]})\n",
    "    inference_set = CustomDataset(inference_dataset, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "    inference_loader = DataLoader(inference_set, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # run model on input data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in inference_loader:\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            output = model(ids, mask, token_type_ids)\n",
    "\n",
    "    # process output\n",
    "    predictions = torch.nn.functional.softmax(output, dim=1)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "\n",
    "    hallmarks = [l.item() for l in predicted_classes.data]\n",
    "\n",
    "    hallmarksDesc = \"\"\n",
    "    for lID in hallmarks:\n",
    "        hallmarksDesc += keys[str(lID)]\n",
    "\n",
    "    print(hallmarksDesc)\n",
    "\n",
    "    return hallmarksDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genome instability & mutation (GI)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genome instability & mutation (GI)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shippai/.pyenv/versions/3.11.1/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genome instability & mutation (GI)\n"
     ]
    }
   ],
   "source": [
    "# input_text = input('Express your symptoms in a sentence:') #ex- 'There was no evidence of immunosuppression.'\n",
    "# output = inference(input_text)\n",
    "# print(f'hallmark: {output}')\n",
    "\n",
    "iface = gr.Interface(fn=inference, inputs='text', outputs='text', title='Hallmarks of Cancer')\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
